{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e40415",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Basics\n",
    "\n",
    "This notebook covers the fundamental PyTorch tensor operations and conversions:\n",
    "\n",
    "## ðŸ“š **What You'll Learn**\n",
    "\n",
    "- **Tensor Creation**: Various ways to create PyTorch tensors\n",
    "- **Tensor Operations**: Matrix multiplication, element-wise operations\n",
    "- **Shape Manipulation**: Converting between 1D tensors and 2D row/column vectors\n",
    "- **NumPy Integration**: Converting between PyTorch tensors and NumPy arrays\n",
    "- **Memory Management**: Understanding memory sharing and copying\n",
    "\n",
    "## ðŸŽ¯ **Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How PyTorch tensors compare to NumPy arrays\n",
    "- The difference between `view()`, `reshape()`, and `unsqueeze()`\n",
    "- Memory sharing between tensors and NumPy arrays\n",
    "- Essential tensor operations for deep learning\n",
    "\n",
    "Let's explore PyTorch tensors! ðŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d89a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Print versions\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"Random seeds set for reproducible results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccadb426",
   "metadata": {},
   "source": [
    "## Tensor Creation Methods\n",
    "\n",
    "PyTorch provides several ways to create tensors. Let's explore the most common methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eecadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating tensors from lists\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"From list:\", tensor_from_list)\n",
    "print(\"Shape:\", tensor_from_list.shape)\n",
    "print(\"Data type:\", tensor_from_list.dtype)\n",
    "\n",
    "# 2. Creating 2D tensors (matrices)\n",
    "matrix_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "print(\"\\n2D tensor:\", matrix_tensor)\n",
    "print(\"Shape:\", matrix_tensor.shape)\n",
    "\n",
    "# 3. Creating tensors filled with zeros\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "print(\"\\nZeros tensor:\", zeros_tensor)\n",
    "\n",
    "# 4. Creating tensors filled with ones\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(\"\\nOnes tensor:\", ones_tensor)\n",
    "\n",
    "# 5. Creating tensors with random values (uniform distribution [0, 1))\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "print(\"\\nRandom tensor (uniform [0,1)):\", rand_tensor)\n",
    "\n",
    "# 6. Creating tensors with random values (normal distribution, mean=0, std=1)\n",
    "randn_tensor = torch.randn(2, 3)\n",
    "print(\"\\nRandom tensor (normal):\", randn_tensor)\n",
    "\n",
    "# 7. Creating tensors with specific values\n",
    "full_tensor = torch.full((2, 3), 7.5)\n",
    "print(\"\\nFilled with 7.5:\", full_tensor)\n",
    "\n",
    "# 8. Creating identity matrix\n",
    "identity_tensor = torch.eye(3)\n",
    "print(\"\\nIdentity matrix:\", identity_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f991a",
   "metadata": {},
   "source": [
    "## 2D Tensor (Matrix) Operations\n",
    "\n",
    "Let's explore PyTorch tensor operations and see how they compare to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch tensors (same data as NumPy examples)\n",
    "C_torch = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)  # Shape: (2, 3)\n",
    "D_torch = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)  # Shape: (3, 2)\n",
    "\n",
    "print(\"Tensor C:\")\n",
    "print(C_torch)\n",
    "print(f\"Shape: {C_torch.shape}\")  # Note: torch.Size instead of tuple\n",
    "print(f\"Data type: {C_torch.dtype}\\n\")\n",
    "\n",
    "print(\"Tensor D:\")\n",
    "print(D_torch)\n",
    "print(f\"Shape: {D_torch.shape}\\n\")\n",
    "\n",
    "print(\"=== Matrix Multiplication (PyTorch) ===\")\n",
    "result1_torch = C_torch @ D_torch            # Modern syntax (same as NumPy)\n",
    "result2_torch = torch.matmul(C_torch, D_torch)  # Explicit function\n",
    "result3_torch = torch.mm(C_torch, D_torch)     # PyTorch-specific (2D only)\n",
    "\n",
    "print(\"C @ D =\")\n",
    "print(result1_torch)\n",
    "print(f\"Shape: {result1_torch.shape}\")\n",
    "\n",
    "print(f\"\\nAll methods give same result: {torch.allclose(result1_torch, result2_torch) and torch.allclose(result2_torch, result3_torch)}\")\n",
    "\n",
    "print(\"\\n=== Element-wise Operations (PyTorch) ===\")\n",
    "element_wise_torch = C_torch * D_torch.T  # Element-wise with transpose\n",
    "print(\"C * D.T (element-wise):\")\n",
    "print(element_wise_torch)\n",
    "\n",
    "# Using torch.mul (equivalent to *)\n",
    "print(\"\\ntorch.mul(C, D.T):\")\n",
    "print(torch.mul(C_torch, D_torch.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e91085",
   "metadata": {},
   "source": [
    "## 1D Tensor (Vector) Operations\n",
    "\n",
    "PyTorch 1D tensors behave similarly to NumPy 1D arrays, with some additional functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1D tensors\n",
    "v1_torch = torch.tensor([1, 2, 3], dtype=torch.float32)  # Shape: (3,)\n",
    "v2_torch = torch.tensor([4, 5, 6], dtype=torch.float32)  # Shape: (3,)\n",
    "\n",
    "print(\"Vector v1:\", v1_torch, f\"Shape: {v1_torch.shape}\")\n",
    "print(\"Vector v2:\", v2_torch, f\"Shape: {v2_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== Dot Product (PyTorch) ===\")\n",
    "dot1_torch = torch.dot(v1_torch, v2_torch)  # PyTorch dot function\n",
    "dot2_torch = v1_torch @ v2_torch             # @ operator\n",
    "dot3_torch = torch.sum(v1_torch * v2_torch)  # Manual computation\n",
    "\n",
    "print(f\"torch.dot(v1, v2) = {dot1_torch}\")\n",
    "print(f\"v1 @ v2 = {dot2_torch}\")\n",
    "print(f\"Manual: torch.sum(v1 * v2) = {dot3_torch}\")\n",
    "\n",
    "print(\"\\n=== Outer Product (PyTorch) ===\")\n",
    "outer_torch = torch.outer(v1_torch, v2_torch)\n",
    "print(\"torch.outer(v1, v2) =\")\n",
    "print(outer_torch)\n",
    "print(f\"Shape: {outer_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== Element-wise Operations (PyTorch) ===\")\n",
    "element_wise_torch = v1_torch * v2_torch\n",
    "print(f\"v1 * v2 (element-wise) = {element_wise_torch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a900d",
   "metadata": {},
   "source": [
    "## Converting 1D Tensors to Row/Column Vectors\n",
    "\n",
    "PyTorch provides several methods for shape manipulation, similar to NumPy but with some PyTorch-specific additions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a 1D tensor\n",
    "v1_torch = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(f\"Original 1D tensor: {v1_torch}, shape: {v1_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== Method 1: Using unsqueeze (PyTorch-specific) ===\")\n",
    "v1_row_torch = v1_torch.unsqueeze(0)  # Add dimension at index 0 â†’ row vector\n",
    "v1_col_torch = v1_torch.unsqueeze(1)  # Add dimension at index 1 â†’ column vector\n",
    "\n",
    "print(f\"Row tensor: {v1_row_torch}, shape: {v1_row_torch.shape}\")\n",
    "print(f\"Column tensor:\\n{v1_col_torch}, shape: {v1_col_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== Method 2: Using reshape ===\")\n",
    "v1_row2_torch = v1_torch.reshape(1, -1)  # Reshape to (1, n) â†’ row vector\n",
    "v1_col2_torch = v1_torch.reshape(-1, 1)  # Reshape to (n, 1) â†’ column vector\n",
    "\n",
    "print(f\"Row tensor (reshape): {v1_row2_torch}, shape: {v1_row2_torch.shape}\")\n",
    "print(f\"Column tensor (reshape):\\n{v1_col2_torch}, shape: {v1_col2_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== Method 3: Using view (PyTorch-specific) ===\")\n",
    "v1_row3_torch = v1_torch.view(1, -1)  # View as (1, n) â†’ row vector\n",
    "v1_col3_torch = v1_torch.view(-1, 1)  # View as (n, 1) â†’ column vector\n",
    "\n",
    "print(f\"Row tensor (view): {v1_row3_torch}, shape: {v1_row3_torch.shape}\")\n",
    "print(f\"Column tensor (view):\\n{v1_col3_torch}, shape: {v1_col3_torch.shape}\")\n",
    "\n",
    "print(\"\\n=== PyTorch vs NumPy: Key Differences ===\")\n",
    "print(\"1. PyTorch uses unsqueeze() instead of np.newaxis\")\n",
    "print(\"2. PyTorch has view() which is similar to reshape() but stricter\")\n",
    "print(\"3. view() requires contiguous memory, reshape() is more flexible\")\n",
    "\n",
    "print(\"\\n=== Verification: All methods are equivalent ===\")\n",
    "print(f\"All row methods equal: {torch.equal(v1_row_torch, v1_row2_torch) and torch.equal(v1_row2_torch, v1_row3_torch)}\")\n",
    "print(f\"All column methods equal: {torch.equal(v1_col_torch, v1_col2_torch) and torch.equal(v1_col2_torch, v1_col3_torch)}\")\n",
    "\n",
    "print(\"\\n=== Memory Layout: view vs reshape ===\")\n",
    "# Create a tensor and demonstrate view vs reshape\n",
    "original = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "transposed = original.T  # This creates a non-contiguous tensor\n",
    "\n",
    "print(f\"Original tensor is contiguous: {original.is_contiguous()}\")\n",
    "print(f\"Transposed tensor is contiguous: {transposed.is_contiguous()}\")\n",
    "\n",
    "# view() requires contiguous memory\n",
    "try:\n",
    "    viewed = transposed.view(-1)  # This might fail\n",
    "    print(f\"view() worked: {viewed}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"view() failed: {e}\")\n",
    "    # Use contiguous() to fix\n",
    "    viewed = transposed.contiguous().view(-1)\n",
    "    print(f\"view() after contiguous(): {viewed}\")\n",
    "\n",
    "# reshape() handles non-contiguous tensors automatically\n",
    "reshaped = transposed.reshape(-1)\n",
    "print(f\"reshape() always works: {reshaped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d97d3",
   "metadata": {},
   "source": [
    "## Converting Between PyTorch Tensors and NumPy Arrays\n",
    "\n",
    "One of PyTorch's great features is seamless integration with NumPy. Let's see how to convert between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a NumPy array first\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "print(\"Original NumPy array:\")\n",
    "print(np_array)\n",
    "print(\"Type:\", type(np_array))\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(\"\\nConverted to PyTorch tensor:\")\n",
    "print(torch_tensor)\n",
    "print(\"Type:\", type(torch_tensor))\n",
    "\n",
    "# IMPORTANT: torch.from_numpy() creates a tensor that shares memory with the NumPy array\n",
    "print(\"\\n--- Memory Sharing Demonstration ---\")\n",
    "print(\"Original NumPy array:\", np_array)\n",
    "print(\"Original tensor:\", torch_tensor)\n",
    "\n",
    "# Modify the NumPy array\n",
    "np_array[0, 0] = 999\n",
    "print(\"\\nAfter modifying NumPy array:\")\n",
    "print(\"NumPy array:\", np_array)\n",
    "print(\"Tensor (also changed!):\", torch_tensor)  # Notice it changed too!\n",
    "\n",
    "# Reset for next examples  \n",
    "np_array[0, 0] = 1\n",
    "\n",
    "# Convert PyTorch tensor back to NumPy array\n",
    "numpy_from_torch = torch_tensor.numpy()\n",
    "print(\"Converted back to NumPy:\")\n",
    "print(numpy_from_torch)\n",
    "print(\"Type:\", type(numpy_from_torch))\n",
    "\n",
    "# Alternative: Create a copy (no memory sharing)\n",
    "np_array_copy = np.array([[7, 8, 9], [10, 11, 12]], dtype=np.float32)\n",
    "torch_tensor_copy = torch.tensor(np_array_copy)  # Note: torch.tensor() creates a copy\n",
    "print(\"\\n--- No Memory Sharing (using torch.tensor()) ---\")\n",
    "print(\"Original NumPy:\", np_array_copy)\n",
    "print(\"Tensor copy:\", torch_tensor_copy)\n",
    "\n",
    "np_array_copy[0, 0] = 777\n",
    "print(\"\\nAfter modifying NumPy array:\")\n",
    "print(\"NumPy array:\", np_array_copy)\n",
    "print(\"Tensor (unchanged!):\", torch_tensor_copy)  # This one doesn't change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f07148",
   "metadata": {},
   "source": [
    "### Memory Sharing vs. Copying: Best Practices\n",
    "\n",
    "**Key Points:**\n",
    "- `torch.from_numpy()` shares memory with the original NumPy array\n",
    "- `torch.tensor()` creates a copy, no memory sharing\n",
    "- `.numpy()` on CPU tensors shares memory with the tensor\n",
    "- For GPU tensors, use `.cpu().numpy()` to convert back to NumPy\n",
    "\n",
    "**When to use each:**\n",
    "- Use `torch.from_numpy()` when you want efficient conversion without extra memory\n",
    "- Use `torch.tensor()` when you need independent copies that won't affect each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type compatibility examples\n",
    "print(\"=== Data Type Compatibility ===\")\n",
    "\n",
    "# Different NumPy data types\n",
    "np_int32 = np.array([1, 2, 3], dtype=np.int32)\n",
    "np_float64 = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "np_bool = np.array([True, False, True], dtype=bool)\n",
    "\n",
    "torch_from_int32 = torch.from_numpy(np_int32)\n",
    "torch_from_float64 = torch.from_numpy(np_float64)\n",
    "torch_from_bool = torch.from_numpy(np_bool)\n",
    "\n",
    "print(f\"NumPy int32 -> PyTorch: {torch_from_int32.dtype}\")\n",
    "print(f\"NumPy float64 -> PyTorch: {torch_from_float64.dtype}\")\n",
    "print(f\"NumPy bool -> PyTorch: {torch_from_bool.dtype}\")\n",
    "\n",
    "# Convert back to NumPy\n",
    "print(f\"\\nPyTorch -> NumPy types:\")\n",
    "print(f\"int32 tensor -> NumPy: {torch_from_int32.numpy().dtype}\")\n",
    "print(f\"float64 tensor -> NumPy: {torch_from_float64.numpy().dtype}\")\n",
    "print(f\"bool tensor -> NumPy: {torch_from_bool.numpy().dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d4105",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Tensor Creation**: Various methods to create PyTorch tensors\n",
    "2. **2D Operations**: Matrix multiplication and element-wise operations\n",
    "3. **1D Operations**: Vector operations (dot product, outer product)\n",
    "4. **Shape Manipulation**: Using `unsqueeze()`, `reshape()`, and `view()`\n",
    "5. **NumPy Integration**: Converting between PyTorch tensors and NumPy arrays\n",
    "\n",
    "### Important Notes:\n",
    "- PyTorch tensors are similar to NumPy arrays but with GPU support and automatic differentiation\n",
    "- Memory sharing between NumPy and PyTorch can be beneficial for performance but requires careful handling\n",
    "- Shape manipulation is essential for preparing data for neural networks\n",
    "- Understanding tensor operations is fundamental for deep learning implementations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
