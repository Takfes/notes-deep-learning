{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16b04da",
   "metadata": {},
   "source": [
    "# PyTorch DAGs, Gradients, and Detachment\n",
    "\n",
    "This notebook covers PyTorch's automatic differentiation system and computation graph management:\n",
    "\n",
    "## üìö **What You'll Learn**\n",
    "\n",
    "- **Computation Graph (DAG)**: Understanding PyTorch's directed acyclic graph\n",
    "- **Gradient Tracking**: Enabling and disabling automatic differentiation\n",
    "- **Tensor Detachment**: Breaking gradient flow and memory management\n",
    "- **Autograd System**: How PyTorch computes gradients automatically\n",
    "\n",
    "## üéØ **Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How PyTorch builds computation graphs for backpropagation\n",
    "- When and how to enable/disable gradient tracking\n",
    "- The difference between leaf and non-leaf tensors\n",
    "- How to use `.detach()` for memory management and gradient control\n",
    "\n",
    "This is the foundation of how neural networks learn! üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6cc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "torch.manual_seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515caf1",
   "metadata": {},
   "source": [
    "## Inspecting the Computation Graph (DAG)\n",
    "\n",
    "PyTorch builds a Directed Acyclic Graph (DAG) to track operations for automatic differentiation. Let's explore how to inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96479e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors that will be part of computation graph\n",
    "print(\"=== Building a Computation Graph ===\")\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([[3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "\n",
    "# Check initial gradient functions (should be None for leaf tensors)\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")  # None - this is a leaf tensor\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")  # None - this is a leaf tensor\n",
    "print(f\"x.is_leaf: {x.is_leaf}\")  # True - created by user\n",
    "print(f\"y.is_leaf: {y.is_leaf}\")  # True - created by user\n",
    "\n",
    "print(\"\\n=== Operations Create Graph Nodes ===\")\n",
    "\n",
    "# Perform operations - each creates a node in the computation graph\n",
    "z = x + y  # AddBackward\n",
    "print(f\"z = x + y: {z}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "print(f\"z.requires_grad: {z.requires_grad}\")\n",
    "print(f\"z.is_leaf: {z.is_leaf}\")  # False - computed from other tensors\n",
    "\n",
    "w = z * 2  # MulBackward\n",
    "print(f\"w = z * 2: {w}\")\n",
    "print(f\"w.grad_fn: {w.grad_fn}\")\n",
    "\n",
    "result = w.sum()  # SumBackward\n",
    "print(f\"result = w.sum(): {result}\")\n",
    "print(f\"result.grad_fn: {result.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Exploring the Graph Structure ===\")\n",
    "\n",
    "# You can access the next functions in the graph\n",
    "print(f\"result.grad_fn.next_functions: {result.grad_fn.next_functions}\")\n",
    "\n",
    "# Each next_function is a tuple of (function, input_nr)\n",
    "for i, (func, input_nr) in enumerate(result.grad_fn.next_functions):\n",
    "    print(f\"  Function {i}: {func}, Input number: {input_nr}\")\n",
    "\n",
    "print(\"\\n=== More Complex Example ===\")\n",
    "\n",
    "# Reset and create a more complex computation\n",
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Build computation: result = (a * b).sum() + (a ** 2).mean()\n",
    "c = a * b              # MulBackward\n",
    "d = a ** 2             # PowBackward  \n",
    "e = c.sum()            # SumBackward\n",
    "f = d.mean()           # MeanBackward\n",
    "result2 = e + f        # AddBackward\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c = a * b: {c}, grad_fn: {c.grad_fn}\")\n",
    "print(f\"d = a ** 2: {d}, grad_fn: {d.grad_fn}\")\n",
    "print(f\"e = c.sum(): {e}, grad_fn: {e.grad_fn}\")\n",
    "print(f\"f = d.mean(): {f}, grad_fn: {f.grad_fn}\")\n",
    "print(f\"result2 = e + f: {result2}, grad_fn: {result2.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a05f2",
   "metadata": {},
   "source": [
    "## Gradient Tracking and Computation\n",
    "\n",
    "Let's explore how PyTorch tracks and computes gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with and without gradient tracking\n",
    "print(\"=== Creating Tensors with Gradient Tracking ===\")\n",
    "\n",
    "# Default: no gradient tracking\n",
    "tensor_no_grad = torch.randn(2, 2)\n",
    "print(f\"Default tensor requires_grad: {tensor_no_grad.requires_grad}\")\n",
    "\n",
    "# Explicitly enable gradient tracking\n",
    "tensor_with_grad = torch.randn(2, 2, requires_grad=True)\n",
    "print(f\"Explicit requires_grad tensor: {tensor_with_grad.requires_grad}\")\n",
    "\n",
    "# Enable gradient tracking on existing tensor\n",
    "tensor_no_grad.requires_grad_(True)  # In-place modification\n",
    "print(f\"Modified tensor requires_grad: {tensor_no_grad.requires_grad}\")\n",
    "\n",
    "print(\"\\n=== Disabling Gradients Temporarily ===\")\n",
    "\n",
    "# Using torch.no_grad() context manager\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(f\"y.requires_grad (inside no_grad): {y.requires_grad}\")\n",
    "\n",
    "# Outside the context, gradients work normally\n",
    "z = x * 3\n",
    "print(f\"z.requires_grad (outside no_grad): {z.requires_grad}\")\n",
    "\n",
    "print(\"\\n=== Computing Gradients ===\")\n",
    "\n",
    "# Simple gradient computation example\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x.pow(2).sum()  # y = x‚ÇÅ¬≤ + x‚ÇÇ¬≤\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"x.grad before backward(): {x.grad}\")\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(f\"x.grad after backward(): {x.grad}\")\n",
    "print(\"Expected: [2*x‚ÇÅ, 2*x‚ÇÇ] = [4.0, 6.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdc815",
   "metadata": {},
   "source": [
    "## Tensor Detachment and Memory Management\n",
    "\n",
    "The `.detach()` method is crucial for controlling gradient flow and memory management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e00ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== What is .detach()? ===\")\n",
    "print(\"detach() creates a new tensor that shares the same data but is removed from the computation graph\")\n",
    "print(\"- Same data (shares memory)\")\n",
    "print(\"- No gradient tracking (requires_grad=False)\")\n",
    "print(\"- Not connected to computation graph\")\n",
    "\n",
    "# Example 1: Basic detachment\n",
    "print(\"\\n=== Basic Detachment ===\")\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y + 1\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"z: {z}\")\n",
    "print(f\"z.requires_grad: {z.requires_grad}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "\n",
    "# Detach z\n",
    "z_detached = z.detach()\n",
    "print(f\"\\nAfter detachment:\")\n",
    "print(f\"z_detached: {z_detached}\")\n",
    "print(f\"z_detached.requires_grad: {z_detached.requires_grad}\")\n",
    "print(f\"z_detached.grad_fn: {z_detached.grad_fn}\")\n",
    "\n",
    "# Verify they share the same data\n",
    "print(f\"\\nShared memory check:\")\n",
    "print(f\"Same data: {torch.equal(z, z_detached)}\")\n",
    "print(f\"z.data_ptr(): {z.data_ptr()}\")\n",
    "print(f\"z_detached.data_ptr(): {z_detached.data_ptr()}\")\n",
    "print(f\"Same memory location: {z.data_ptr() == z_detached.data_ptr()}\")\n",
    "\n",
    "print(\"\\n=== Detach vs Clone vs Copy ===\")\n",
    "\n",
    "original = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "print(f\"original: {original}, requires_grad: {original.requires_grad}\")\n",
    "\n",
    "# Method 1: detach() - shares memory, no gradients\n",
    "detached = original.detach()\n",
    "print(f\"detached: {detached}, requires_grad: {detached.requires_grad}\")\n",
    "\n",
    "# Method 2: clone() - new memory, keeps gradients\n",
    "cloned = original.clone()\n",
    "print(f\"cloned: {cloned}, requires_grad: {cloned.requires_grad}\")\n",
    "\n",
    "# Method 3: detach().clone() - new memory, no gradients\n",
    "detached_cloned = original.detach().clone()\n",
    "print(f\"detached_cloned: {detached_cloned}, requires_grad: {detached_cloned.requires_grad}\")\n",
    "\n",
    "# Demonstrate memory sharing\n",
    "print(f\"\\nMemory sharing:\")\n",
    "print(f\"original ptr: {original.data_ptr()}\")\n",
    "print(f\"detached ptr: {detached.data_ptr()} (same: {original.data_ptr() == detached.data_ptr()})\")\n",
    "print(f\"cloned ptr: {cloned.data_ptr()} (same: {original.data_ptr() == cloned.data_ptr()})\")\n",
    "print(f\"detached_cloned ptr: {detached_cloned.data_ptr()} (same: {original.data_ptr() == detached_cloned.data_ptr()})\")\n",
    "\n",
    "print(\"\\n=== When to Use .detach() ===\")\n",
    "use_cases = [\n",
    "    \"1. Converting tensor to NumPy: tensor.detach().numpy()\",\n",
    "    \"2. Stopping gradient flow in part of computation\",\n",
    "    \"3. Creating inputs for inference without gradients\",\n",
    "    \"4. Memory optimization by breaking graph references\",\n",
    "    \"5. Debugging: isolating parts of computation graph\"\n",
    "]\n",
    "\n",
    "for use_case in use_cases:\n",
    "    print(use_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34442a3",
   "metadata": {},
   "source": [
    "## Practical Examples: Real-world Usage\n",
    "\n",
    "Let's see some practical examples of DAGs, gradients, and detachment in deep learning contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple neural network forward pass with gradient tracking\n",
    "print(\"=== Example 1: Neural Network Forward Pass ===\")\n",
    "\n",
    "# Simulate a simple linear layer: y = Wx + b\n",
    "W = torch.randn(3, 2, requires_grad=True)  # Weight matrix\n",
    "b = torch.randn(3, requires_grad=True)     # Bias vector\n",
    "x = torch.randn(5, 2)                      # Input batch (5 samples, 2 features)\n",
    "\n",
    "print(f\"W shape: {W.shape}, requires_grad: {W.requires_grad}\")\n",
    "print(f\"b shape: {b.shape}, requires_grad: {b.requires_grad}\")\n",
    "print(f\"x shape: {x.shape}, requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Forward pass\n",
    "y = torch.matmul(x, W.T) + b  # Linear transformation\n",
    "print(f\"y shape: {y.shape}, requires_grad: {y.requires_grad}\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "\n",
    "# Apply activation function\n",
    "z = torch.relu(y)\n",
    "print(f\"z (after ReLU): {z.shape}, requires_grad: {z.requires_grad}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "\n",
    "# Compute loss (mean squared error with target)\n",
    "target = torch.randn(5, 3)\n",
    "loss = torch.mean((z - target) ** 2)\n",
    "print(f\"loss: {loss.item():.4f}, requires_grad: {loss.requires_grad}\")\n",
    "print(f\"loss.grad_fn: {loss.grad_fn}\")\n",
    "\n",
    "print(\"\\n=== Example 2: Training Step with Gradients ===\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"W.grad shape: {W.grad.shape}\")\n",
    "print(f\"W.grad:\\n{W.grad}\")\n",
    "print(f\"b.grad shape: {b.grad.shape}\")\n",
    "print(f\"b.grad: {b.grad}\")\n",
    "\n",
    "# Simulate optimizer step (gradient descent)\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():  # Disable gradient tracking for parameter updates\n",
    "    W -= learning_rate * W.grad\n",
    "    b -= learning_rate * b.grad\n",
    "\n",
    "print(\"Parameters updated with gradient descent\")\n",
    "\n",
    "# Clear gradients for next iteration\n",
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(\"Gradients cleared for next iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70127493",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "#### Computation Graph (DAG):\n",
    "- **DAG Structure**: PyTorch builds a Directed Acyclic Graph to track operations\n",
    "- **Node Types**: Leaf tensors (created by user) vs intermediate tensors (computed)\n",
    "- **Graph Functions**: Each operation creates a `grad_fn` for backpropagation\n",
    "- **Graph Inspection**: Use `.grad_fn`, `.is_leaf`, and `.next_functions` to explore\n",
    "\n",
    "#### Gradient Tracking:\n",
    "- **requires_grad**: Controls whether gradients are computed for a tensor\n",
    "- **Gradient Flow**: Only tensors with `requires_grad=True` participate in gradients\n",
    "- **Context Managers**: Use `torch.no_grad()` to temporarily disable gradients\n",
    "- **Backward Pass**: Call `.backward()` to compute gradients through the graph\n",
    "\n",
    "#### Tensor Detachment:\n",
    "- **Memory Sharing**: `.detach()` creates new tensor sharing same data\n",
    "- **Gradient Breaking**: Detached tensors have `requires_grad=False`\n",
    "- **Use Cases**: Converting to NumPy, stopping gradient flow, memory optimization\n",
    "- **Alternatives**: `.clone()` for new memory, `.detach().clone()` for both\n",
    "\n",
    "### Best Practices:\n",
    "1. **Enable gradients** only for parameters that need training\n",
    "2. **Use `torch.no_grad()`** during inference to save memory\n",
    "3. **Clear gradients** with `.zero_()` between training steps\n",
    "4. **Detach tensors** when moving data between training components\n",
    "5. **Monitor memory usage** in complex graphs with many intermediate tensors\n",
    "\n",
    "Understanding these concepts is essential for debugging PyTorch models and optimizing training performance! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
