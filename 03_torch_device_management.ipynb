{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fbe8c1",
   "metadata": {},
   "source": [
    "# PyTorch Device Management\n",
    "\n",
    "This notebook covers how to work with different computational devices in PyTorch:\n",
    "\n",
    "## üìö **What You'll Learn**\n",
    "\n",
    "- **Device Detection**: How to check for available devices (CPU, CUDA, MPS)\n",
    "- **Moving Tensors**: How to move tensors between devices\n",
    "- **Device Compatibility**: Understanding device requirements for operations\n",
    "- **Performance Optimization**: Best practices for device usage\n",
    "- **Memory Management**: Efficient tensor creation and movement\n",
    "\n",
    "## üéØ **Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to detect CUDA (NVIDIA GPU) and MPS (Apple Silicon) availability\n",
    "- Different methods to move tensors between devices\n",
    "- Why tensors must be on the same device for operations\n",
    "- How to optimize performance with proper device usage\n",
    "\n",
    "Let's master PyTorch device management! ‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee187634",
   "metadata": {},
   "source": [
    "## Checking Available Devices\n",
    "\n",
    "Let's check what computational devices are available on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0175cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CPU (always available)\n",
    "print(\"CPU is available: True (always)\")\n",
    "\n",
    "# Check CUDA (NVIDIA GPU)\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA is available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Check MPS (Apple Silicon GPU)\n",
    "# Metal Performance Shaders (MPS) for Apple Silicon\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "print(f\"MPS is available: {mps_available}\")\n",
    "\n",
    "# Check if MPS is built (compiled with MPS support)\n",
    "mps_built = torch.backends.mps.is_built()\n",
    "print(f\"MPS is built: {mps_built}\")\n",
    "\n",
    "# Determine the best available device\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\\nBest available device: {device}\")\n",
    "elif mps_available:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"\\nBest available device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"\\nBest available device: {device}\")\n",
    "\n",
    "# Create a tensor and check its default device\n",
    "sample_tensor = torch.randn(3, 3)\n",
    "print(f\"\\nDefault tensor device: {sample_tensor.device}\")\n",
    "print(f\"Default tensor device type: {sample_tensor.device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e4541",
   "metadata": {},
   "source": [
    "## Moving Tensors Between Devices\n",
    "\n",
    "Now let's see how to move tensors between different devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619528a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor on CPU (default)\n",
    "cpu_tensor = torch.randn(3, 3)\n",
    "print(\"Original tensor device:\", cpu_tensor.device)\n",
    "print(\"Tensor values:\\n\", cpu_tensor)\n",
    "\n",
    "# Method 1: Using .to() with device string\n",
    "cpu_tensor_moved = cpu_tensor.to('cpu')  # Already on CPU, but showing the syntax\n",
    "print(f\"\\nUsing .to('cpu'): {cpu_tensor_moved.device}\")\n",
    "\n",
    "# Method 2: Using .to() with device object\n",
    "device_obj = torch.device('cpu')\n",
    "cpu_tensor_moved2 = cpu_tensor.to(device_obj)\n",
    "print(f\"Using .to(device_obj): {cpu_tensor_moved2.device}\")\n",
    "\n",
    "# Method 3: Using specific device methods\n",
    "cpu_tensor_moved3 = cpu_tensor.cpu()  # Explicit CPU method\n",
    "print(f\"Using .cpu(): {cpu_tensor_moved3.device}\")\n",
    "\n",
    "# Try moving to CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- CUDA Examples ---\")\n",
    "    cuda_tensor = cpu_tensor.to('cuda')\n",
    "    print(f\"Moved to CUDA: {cuda_tensor.device}\")\n",
    "    \n",
    "    # You can also specify which GPU (if multiple)\n",
    "    cuda_tensor_gpu0 = cpu_tensor.to('cuda:0')\n",
    "    print(f\"Moved to CUDA:0: {cuda_tensor_gpu0.device}\")\n",
    "    \n",
    "    # Using .cuda() method\n",
    "    cuda_tensor2 = cpu_tensor.cuda()\n",
    "    print(f\"Using .cuda(): {cuda_tensor2.device}\")\n",
    "    \n",
    "    # Move back to CPU\n",
    "    back_to_cpu = cuda_tensor.to('cpu')\n",
    "    print(f\"Back to CPU: {back_to_cpu.device}\")\n",
    "    \n",
    "    # Perform computation on GPU\n",
    "    gpu_result = cuda_tensor @ cuda_tensor.T  # Matrix multiplication on GPU\n",
    "    print(f\"GPU computation result device: {gpu_result.device}\")\n",
    "    print(f\"Result shape: {gpu_result.shape}\")\n",
    "\n",
    "# Try moving to MPS if available\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"\\n--- MPS Examples ---\")\n",
    "    mps_tensor = cpu_tensor.to('mps')\n",
    "    print(f\"Moved to MPS: {mps_tensor.device}\")\n",
    "    \n",
    "    # Perform computation on MPS\n",
    "    mps_result = mps_tensor @ mps_tensor.T\n",
    "    print(f\"MPS computation result device: {mps_result.device}\")\n",
    "    print(f\"Result shape: {mps_result.shape}\")\n",
    "    \n",
    "    # Move back to CPU\n",
    "    back_to_cpu_from_mps = mps_tensor.to('cpu')\n",
    "    print(f\"Back to CPU from MPS: {back_to_cpu_from_mps.device}\")\n",
    "\n",
    "# IMPORTANT: Tensors must be on the same device for operations!\n",
    "print(\"\\n--- Device Compatibility ---\")\n",
    "tensor_a = torch.randn(2, 2)  # CPU\n",
    "tensor_b = torch.randn(2, 2)  # CPU\n",
    "\n",
    "print(f\"tensor_a device: {tensor_a.device}\")\n",
    "print(f\"tensor_b device: {tensor_b.device}\")\n",
    "\n",
    "# This works - both on CPU\n",
    "result_cpu = tensor_a @ tensor_b\n",
    "print(\"CPU @ CPU: Success\")\n",
    "\n",
    "# This would fail if tensors are on different devices:\n",
    "# If tensor_a is on CPU and tensor_b is on CUDA, tensor_a @ tensor_b would raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b4b2c",
   "metadata": {},
   "source": [
    "## Device Compatibility and Operations\n",
    "\n",
    "**Important:** Tensors must be on the same device to perform operations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on different devices\n",
    "cpu_tensor1 = torch.randn(3, 3)\n",
    "cpu_tensor2 = torch.randn(3, 3)\n",
    "\n",
    "print(\"Both tensors on CPU:\")\n",
    "print(f\"Tensor 1 device: {cpu_tensor1.device}\")\n",
    "print(f\"Tensor 2 device: {cpu_tensor2.device}\")\n",
    "\n",
    "# This works - both on CPU\n",
    "result_cpu = cpu_tensor1 + cpu_tensor2\n",
    "print(f\"Addition result device: {result_cpu.device}\")\n",
    "\n",
    "# Demonstrate device mismatch error (only if GPU/MPS available)\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    # Determine available accelerated device\n",
    "    if torch.cuda.is_available():\n",
    "        accelerated_device = 'cuda'\n",
    "    else:\n",
    "        accelerated_device = 'mps'\n",
    "    \n",
    "    print(f\"\\n=== Device Mismatch Example (using {accelerated_device}) ===\")\n",
    "    \n",
    "    # Move one tensor to accelerated device\n",
    "    accelerated_tensor = cpu_tensor1.to(accelerated_device)\n",
    "    print(f\"CPU tensor device: {cpu_tensor2.device}\")\n",
    "    print(f\"Accelerated tensor device: {accelerated_tensor.device}\")\n",
    "    \n",
    "    # This would cause an error - demonstrate how to fix it\n",
    "    try:\n",
    "        # This line would fail: result = cpu_tensor2 + accelerated_tensor\n",
    "        print(\"‚ùå Cannot perform operations between tensors on different devices\")\n",
    "        print(\"üí° Solution: Move tensors to the same device\")\n",
    "        \n",
    "        # Solution 1: Move accelerated tensor back to CPU\n",
    "        result1 = cpu_tensor2 + accelerated_tensor.cpu()\n",
    "        print(f\"‚úÖ Solution 1 - Move to CPU: {result1.device}\")\n",
    "        \n",
    "        # Solution 2: Move CPU tensor to accelerated device\n",
    "        result2 = cpu_tensor2.to(accelerated_device) + accelerated_tensor\n",
    "        print(f\"‚úÖ Solution 2 - Move to {accelerated_device}: {result2.device}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"RuntimeError: {e}\")\n",
    "else:\n",
    "    print(\"\\n=== Device Mismatch Example ===\")\n",
    "    print(\"Only CPU available - device mismatch errors occur when tensors are on different devices\")\n",
    "    print(\"Example: CPU tensor + CUDA tensor would fail\")\n",
    "    print(\"Solution: Always ensure tensors are on the same device before operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9db29",
   "metadata": {},
   "source": [
    "## Best Practices for Device Management\n",
    "\n",
    "### 1. Automatic Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Automatic device selection\n",
    "def get_device():\n",
    "    \"\"\"Select the best available device automatically\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Use the function\n",
    "device = get_device()\n",
    "print(f\"Selected device: {device}\")\n",
    "\n",
    "# Create tensors directly on the selected device\n",
    "tensor_on_device = torch.randn(3, 3, device=device)\n",
    "print(f\"Tensor created on: {tensor_on_device.device}\")\n",
    "\n",
    "# Alternative: Create on CPU then move\n",
    "cpu_tensor = torch.randn(3, 3)\n",
    "tensor_moved = cpu_tensor.to(device)\n",
    "print(f\"Tensor moved to: {tensor_moved.device}\")\n",
    "\n",
    "print(\"\\n=== Best Practices Summary ===\")\n",
    "practices = [\n",
    "    \"1. Use automatic device selection functions\",\n",
    "    \"2. Create tensors directly on target device when possible\",\n",
    "    \"3. Ensure all tensors in an operation are on the same device\",\n",
    "    \"4. Move model and data to the same device before training\",\n",
    "    \"5. Use .cpu() to move tensors back for NumPy conversion\",\n",
    "    \"6. Consider memory limitations when using GPU devices\"\n",
    "]\n",
    "\n",
    "for practice in practices:\n",
    "    print(practice)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
